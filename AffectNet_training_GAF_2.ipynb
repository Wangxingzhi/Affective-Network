{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/miniconda3/envs/pose_pytorch/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os,torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import argparse\n",
    "import torchfile\n",
    "from PIL import Image\n",
    "from torchvision import datasets\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotiWDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_filelist, face_filelist, coordinates_filelist, maxFaces):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            filelist: List of names of image/feature files.\n",
    "            root_dir: Dataset directory\n",
    "            transform (callable, optional): Optional transformer to be applied\n",
    "                                            on an image sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.image_filelist = image_filelist\n",
    "        self.face_filelist = face_filelist\n",
    "        \n",
    "        neg_filelist = sorted(os.listdir(image_filelist + 'Negative/'))\n",
    "        neu_filelist = sorted(os.listdir(image_filelist + 'Neutral/'))\n",
    "        pos_filelist = sorted(os.listdir(image_filelist + 'Positive/'))\n",
    "        \n",
    "        all_filelist = neg_filelist + neu_filelist + pos_filelist\n",
    "        \n",
    "        self.name_filelist = [x.split('.')[0] for x in all_filelist]\n",
    "\n",
    "        self.label = []\n",
    "        neg_label = np.array(np.zeros(len(neg_filelist)),dtype = np.int64)\n",
    "        neu_label = np.array(np.ones(len(neu_filelist)),dtype = np.int64)\n",
    "        pos_label = np.array(2*np.ones(len(pos_filelist)),dtype = np.int64)\n",
    "        \n",
    "        self.label.extend(neg_label)\n",
    "        self.label.extend(neu_label)\n",
    "        self.label.extend(pos_label)\n",
    "        \n",
    "        self.file_paths = []\n",
    "        \n",
    "        for f in neg_filelist:\n",
    "            path = os.path.join(self.image_filelist,'Negative/',f)\n",
    "            self.file_paths.append(path)\n",
    "        for f in neu_filelist:\n",
    "            path = os.path.join(self.image_filelist,'Neutral/',f)\n",
    "            self.file_paths.append(path)\n",
    "        for f in pos_filelist:\n",
    "            path = os.path.join(self.image_filelist,'Positive/',f)\n",
    "            self.file_paths.append(path)       \n",
    "\n",
    "        neg_face_path = []\n",
    "        neu_face_path = []\n",
    "        pos_face_path = []\n",
    "        \n",
    "        self.all_face_path = []\n",
    "        \n",
    "        neg_path_filelist = [x.split('.')[0] for x in neg_filelist]\n",
    "        neu_path_filelist = [x.split('.')[0] for x in neu_filelist]\n",
    "        pos_path_filelist = [x.split('.')[0] for x in pos_filelist]\n",
    "\n",
    "        for f in neg_path_filelist:      \n",
    "            path = os.path.join(face_filelist,'Negative/',f)\n",
    "            neg_face_path.append(path)    \n",
    "        for f in neu_path_filelist:      \n",
    "            path = os.path.join(face_filelist,'Neutral/',f)\n",
    "            neu_face_path.append(path)    \n",
    "        for f in pos_path_filelist:      \n",
    "            path = os.path.join(face_filelist,'Positive/',f)\n",
    "            pos_face_path.append(path)                \n",
    "        \n",
    "        self.all_face_path = neg_face_path + neu_face_path + pos_face_path\n",
    "        \n",
    "        self.maxFaces = maxFaces\n",
    "\n",
    "        neg_coordinates_path = []\n",
    "        neu_coordinates_path = []\n",
    "        pos_coordinates_path = []\n",
    "        self.all_coordinates_path = []\n",
    "        for f in neg_path_filelist:      \n",
    "            path = os.path.join(coordinates_filelist,'Negative/',f)\n",
    "            neg_coordinates_path.append(path)    \n",
    "        for f in neu_path_filelist:      \n",
    "            path = os.path.join(coordinates_filelist,'Neutral/',f)\n",
    "            neu_coordinates_path.append(path)    \n",
    "        for f in pos_path_filelist:      \n",
    "            path = os.path.join(coordinates_filelist,'Positive/',f)\n",
    "            pos_coordinates_path.append(path)          \n",
    "            \n",
    "        self.all_coordinates_path = neg_coordinates_path + neu_coordinates_path + pos_coordinates_path        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.file_paths)) \n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        maxFaces = self.maxFaces\n",
    "        #CROPPED FACE IMAGES\n",
    "        face_features = np.zeros((maxFaces, 4096), dtype = 'float32')\n",
    "        faces_coordinate = np.zeros((maxFaces,4), dtype = 'float32')\n",
    "        \n",
    "        counter = 0\n",
    "        for i in range(maxFaces):\n",
    "            face_path = self.all_face_path[idx] + '_' + str(i) + '.npz'  \n",
    "            if os.path.exists(face_path) is False:\n",
    "                break\n",
    "            coordinate = np.load(self.all_coordinates_path[idx] + '_' + str(i) + '.npz')\n",
    "            f_fea = np.load(face_path)\n",
    "            face_feature = f_fea['faces_feature']  \n",
    "            faces_coordinate[i] = coordinate['normalize_data']\n",
    "            face_features[i] = face_feature\n",
    "            counter = counter + 1\n",
    "\n",
    "        label = self.label[idx]\n",
    "        numberFaces = counter\n",
    "            \n",
    "        #SAMPLE\n",
    "        return face_features, faces_coordinate, label, numberFaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch GAF_2 Training')\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "random.seed(42)\n",
    "classes = ('Negative', 'Neutral', 'Positive')\n",
    "\n",
    "\n",
    "train_dataset = EmotiWDataset(image_filelist='../../Data/GAF_2_Data/Train/', face_filelist='../../Data/GAF_2_crop_faces_features/Train/', coordinates_filelist = '../../Data/Coordinates_Data_low_quality/GAF_2_Train/',maxFaces = 16)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, shuffle=True, batch_size=16, num_workers = 0, pin_memory=True)\n",
    "\n",
    "val_dataset = EmotiWDataset(image_filelist='../../Data/GAF_2_Data/Val/', face_filelist='../../Data/GAF_2_crop_faces_features/Val/',coordinates_filelist = '../../Data/Coordinates_Data_low_quality/GAF_2_Val/', maxFaces = 16)\n",
    "\n",
    "validationloader = DataLoader(val_dataset, shuffle = False, batch_size = 128, num_workers = 0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multi_graph(faces_coordinates):\n",
    "    num_node = faces_coordinates.shape[0]\n",
    "    graph_X = torch.ones(num_node+1,num_node+1)\n",
    "    graph_Y = torch.ones(num_node+1,num_node+1)\n",
    "    graph_WH = torch.ones(num_node+1,num_node+1)\n",
    "    X = faces_coordinates[:,0].unsqueeze(1)\n",
    "    Y = faces_coordinates[:,1].unsqueeze(1)\n",
    "    WH = faces_coordinates[:,2:4]\n",
    "    graph_X[0:num_node,0:num_node] = (1 - torch.norm(X[:, None]-X, dim=2, p=1))\n",
    "    graph_Y[0:num_node,0:num_node] = (1 - torch.norm(Y[:, None]-Y, dim=2, p=1))\n",
    "    graph_WH[0:num_node,0:num_node] = (1 - 1/2*torch.norm(WH[:, None]-WH, dim=2, p=1))\n",
    "    return graph_X, graph_Y, graph_WH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    #Z = AXW\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(GCN,self).__init__()\n",
    "        self.fc1 = nn.Linear(dim_in ,2048,bias=False)\n",
    "        self.fc2 = nn.Linear(2048,dim_out,bias=False)\n",
    "        self.activition = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self,X, A):\n",
    "        X = self.activition(self.fc1(A.mm(X)))\n",
    "        X = self.activition(self.fc2(A.mm(X)))\n",
    "        return X\n",
    "\n",
    "def normalize(A, symmetric=True):\n",
    "    \n",
    "    d = A.sum(1)\n",
    "    if symmetric:\n",
    "        #D = D^-1/2\n",
    "        D = torch.diag(torch.pow(d , -0.5))\n",
    "        return D.mm(A).mm(D)\n",
    "    else :\n",
    "        # D=D^-1\n",
    "        D =torch.diag(torch.pow(d,-1))\n",
    "    return D.mm(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Channel_GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Multi_Channel_GCN, self).__init__()\n",
    "        self.gcnX_layer = GCN(dim_in = 4096, dim_out=1024)\n",
    "        \n",
    "        self.gcnY_layer = GCN(dim_in = 4096, dim_out=1024)\n",
    "\n",
    "        self.gcnWH_layer = GCN(dim_in = 4096, dim_out=1024)\n",
    "        \n",
    "        self.channel_attention_layer = nn.Sequential(nn.Linear(1024*3,3,bias=False), nn.Softmax(dim=0))                \n",
    "        \n",
    "        self.fc_output = nn.Linear(1024,3)\n",
    "        \n",
    "    def forward(self, node_feature, A_X, A_Y, A_WH):\n",
    "        A_X = normalize(A_X)\n",
    "        A_Y = normalize(A_Y)\n",
    "        A_WH = normalize(A_WH)\n",
    "        feature_X = self.gcnX_layer(node_feature, A_X)\n",
    "        feature_Y = self.gcnY_layer(node_feature, A_Y)\n",
    "        feature_WH = self.gcnWH_layer(node_feature, A_WH)\n",
    "        feature_All = torch.cat([feature_X,feature_Y,feature_WH],1)\n",
    "        group_node_feature = feature_All[-1]\n",
    "        weights = self.channel_attention_layer(group_node_feature)\n",
    "        X = weights[0]*feature_X + weights[1]*feature_Y + weights[2]*feature_WH\n",
    "        out = self.fc_output(X[-1])\n",
    "        return out,weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Multi_Channel_GCN()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "\n",
      "Epoch: 1\n",
      "[epoch:1, iter:1] Loss: 1.101 | Acc: 43.750% \n",
      "[epoch:1, iter:2] Loss: 1.096 | Acc: 43.750% \n",
      "[epoch:1, iter:3] Loss: 1.096 | Acc: 41.667% \n",
      "[epoch:1, iter:4] Loss: 1.095 | Acc: 43.750% \n",
      "[epoch:1, iter:5] Loss: 1.092 | Acc: 48.750% \n",
      "[epoch:1, iter:6] Loss: 1.090 | Acc: 52.083% \n",
      "[epoch:1, iter:7] Loss: 1.088 | Acc: 53.571% \n",
      "[epoch:1, iter:8] Loss: 1.087 | Acc: 52.344% \n",
      "[epoch:1, iter:9] Loss: 1.083 | Acc: 54.167% \n",
      "[epoch:1, iter:10] Loss: 1.082 | Acc: 55.000% \n",
      "[epoch:1, iter:11] Loss: 1.082 | Acc: 53.977% \n",
      "[epoch:1, iter:12] Loss: 1.079 | Acc: 55.208% \n",
      "[epoch:1, iter:13] Loss: 1.076 | Acc: 57.212% \n",
      "[epoch:1, iter:14] Loss: 1.072 | Acc: 57.589% \n",
      "[epoch:1, iter:15] Loss: 1.071 | Acc: 57.500% \n",
      "[epoch:1, iter:16] Loss: 1.068 | Acc: 57.812% \n",
      "[epoch:1, iter:17] Loss: 1.067 | Acc: 56.618% \n",
      "[epoch:1, iter:18] Loss: 1.066 | Acc: 57.639% \n",
      "[epoch:1, iter:19] Loss: 1.064 | Acc: 58.224% \n",
      "[epoch:1, iter:20] Loss: 1.062 | Acc: 57.812% \n",
      "[epoch:1, iter:21] Loss: 1.060 | Acc: 58.929% \n",
      "[epoch:1, iter:22] Loss: 1.058 | Acc: 58.974% \n",
      "[epoch:1, iter:23] Loss: 1.056 | Acc: 59.401% \n",
      "[epoch:1, iter:24] Loss: 1.053 | Acc: 60.052% \n",
      "[epoch:1, iter:25] Loss: 1.049 | Acc: 60.652% \n",
      "[epoch:1, iter:26] Loss: 1.049 | Acc: 60.000% \n",
      "[epoch:1, iter:27] Loss: 1.045 | Acc: 61.485% \n",
      "[epoch:1, iter:28] Loss: 1.044 | Acc: 61.745% \n",
      "[epoch:1, iter:29] Loss: 1.044 | Acc: 60.907% \n",
      "[epoch:1, iter:30] Loss: 1.042 | Acc: 61.378% \n",
      "[epoch:1, iter:31] Loss: 1.040 | Acc: 61.414% \n",
      "[epoch:1, iter:32] Loss: 1.038 | Acc: 61.448% \n",
      "[epoch:1, iter:33] Loss: 1.035 | Acc: 62.239% \n",
      "[epoch:1, iter:34] Loss: 1.035 | Acc: 62.063% \n",
      "[epoch:1, iter:35] Loss: 1.033 | Acc: 61.896% \n",
      "[epoch:1, iter:36] Loss: 1.031 | Acc: 62.609% \n",
      "[epoch:1, iter:37] Loss: 1.027 | Acc: 62.881% \n",
      "[epoch:1, iter:38] Loss: 1.024 | Acc: 63.306% \n",
      "[epoch:1, iter:39] Loss: 1.023 | Acc: 63.285% \n",
      "[epoch:1, iter:40] Loss: 1.023 | Acc: 62.951% \n",
      "[epoch:1, iter:41] Loss: 1.020 | Acc: 63.400% \n",
      "[epoch:1, iter:42] Loss: 1.019 | Acc: 63.528% \n",
      "[epoch:1, iter:43] Loss: 1.016 | Acc: 63.596% \n",
      "[epoch:1, iter:44] Loss: 1.015 | Acc: 63.429% \n",
      "[epoch:1, iter:45] Loss: 1.011 | Acc: 63.827% \n",
      "[epoch:1, iter:46] Loss: 1.008 | Acc: 64.344% \n",
      "[epoch:1, iter:47] Loss: 1.007 | Acc: 64.438% \n",
      "[epoch:1, iter:48] Loss: 1.005 | Acc: 64.005% \n",
      "[epoch:1, iter:49] Loss: 1.004 | Acc: 64.359% \n",
      "[epoch:1, iter:50] Loss: 1.002 | Acc: 64.447% \n",
      "[epoch:1, iter:51] Loss: 1.000 | Acc: 64.532% \n",
      "[epoch:1, iter:52] Loss: 1.000 | Acc: 64.251% \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20302/2804089280.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m                         \u001b[0mgroup_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindividual_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0mnode_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindividual_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                         \u001b[0mgraph_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_WH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_multi_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces_coordinates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnumberFaces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                         \u001b[0mgraph_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_WH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_Y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_WH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20302/749179529.py\u001b[0m in \u001b[0;36mgenerate_multi_graph\u001b[0;34m(faces_coordinates)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_node\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_node\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgraph_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_node\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_node\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mgraph_WH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_node\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_node\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaces_coordinates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaces_coordinates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义损失函数和优化方式\n",
    "criterion = nn.CrossEntropyLoss()  #损失函数为交叉熵，多用于多分类问题\n",
    "optimizer = optim.Adam(net.parameters(),lr=1e-5, weight_decay = 1e-4)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "EPOCH = 50\n",
    "\n",
    "# 训练\n",
    "if __name__ == \"__main__\":\n",
    "    best_acc = 75  #2 initializing best test accuracy\n",
    "    print(\"Start Training!\")  # 定义遍历数据集的次数\n",
    "    with open(\"acc.txt\", \"w\") as f:\n",
    "        with open(\"log.txt\", \"w\")as f2:\n",
    "            for epoch in range(EPOCH):\n",
    "                print('\\nEpoch: %d' % (epoch + 1))\n",
    "                net.train()\n",
    "                sum_loss = 0.0\n",
    "                correct = 0.0\n",
    "                total = 0.0\n",
    "                sum_group_loss = 0.0\n",
    "                \n",
    "                for i, data in enumerate(trainloader, 0):\n",
    "                    torch.cuda.empty_cache()\n",
    "                    # prepare the data\n",
    "                    length = len(trainloader)\n",
    "                    \n",
    "                    face_features, faces_coordinates, labels, numberFaces = data\n",
    "                    \n",
    "                    ind = np.where(numberFaces==0)\n",
    "                    clear_labels = np.delete(labels, ind)\n",
    "                    face_features, faces_coordinates, labels, clear_labels = face_features.to(device), faces_coordinates.to(device), labels.to(device), clear_labels.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # forward + backward\n",
    "                    group_outputs = torch.zeros(np.count_nonzero(numberFaces),3).requires_grad_(requires_grad=True).to(device)\n",
    "                    count = 0\n",
    "                    for j in range(labels.shape[0]):          \n",
    "                        if numberFaces[j] == 0:\n",
    "                            continue\n",
    "                        individual_features = face_features[j][0:numberFaces[j]]    \n",
    "                        group_feature = individual_features.mean(0)\n",
    "                        node_features = torch.cat([individual_features, group_feature.unsqueeze(0)],0)\n",
    "                        graph_X, graph_Y, graph_WH = generate_multi_graph(faces_coordinates[j][0:numberFaces[j]])\n",
    "                        graph_X, graph_Y, graph_WH = graph_X.to(device), graph_Y.to(device), graph_WH.to(device)\n",
    "                        \n",
    "                        outputs, weights = net(node_features, graph_X, graph_Y, graph_WH)  \n",
    "                        \n",
    "                        group_outputs[count] = outputs\n",
    "                        count += 1\n",
    "                        \n",
    "                    loss = criterion(group_outputs, clear_labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # print loss and acc.each epoch\n",
    "                    sum_loss += loss.item()\n",
    "                    _, predicted = torch.max(group_outputs.data, 1)\n",
    "                    total += clear_labels.size(0)\n",
    "                    correct += predicted.eq(clear_labels.data).cpu().sum()\n",
    "                    print('[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '\n",
    "                          % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "                    f2.write('%03d  %05d |Loss: %.03f | Acc: %.3f%% '\n",
    "                          % (epoch + 1, (i + 1 + epoch * length), sum_loss / (i + 1), 100. * correct / total))\n",
    "                    f2.write('\\n')\n",
    "                    f2.flush()\n",
    "  \n",
    "                scheduler.step()\n",
    "                torch.cuda.empty_cache()\n",
    "                # test the accuracy after every epoch\n",
    "                print(\"Waiting Test!\")\n",
    "                with torch.no_grad():\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    class_correct = list(0. for j in range(3)) \n",
    "                    class_total = list(0. for j in range(3))\n",
    "                    for data in validationloader:\n",
    "                        net.eval()\n",
    "                        \n",
    "                        face_features, faces_coordinates, labels, numberFaces = data\n",
    "                        \n",
    "                        ind = np.where(numberFaces==0)\n",
    "                        clear_labels = np.delete(labels, ind)\n",
    "        \n",
    "                        face_features, faces_coordinates, labels, clear_labels = face_features.to(device), faces_coordinates.to(device), labels.to(device), clear_labels.to(device)\n",
    "\n",
    "                        \n",
    "                        # forward + backward\n",
    "                        group_outputs = torch.zeros(np.count_nonzero(numberFaces),3).requires_grad_(requires_grad=True).to(device)\n",
    "                        \n",
    "                        count = 0\n",
    "                        for j in range(labels.shape[0]):          \n",
    "                            if numberFaces[j] == 0:\n",
    "                                continue\n",
    "                            individual_features = face_features[j][0:numberFaces[j]]    \n",
    "                            group_feature = individual_features.mean(0)\n",
    "                            node_features = torch.cat([individual_features, group_feature.unsqueeze(0)],0)\n",
    "                            graph_X, graph_Y, graph_WH = generate_multi_graph(faces_coordinates[j][0:numberFaces[j]])\n",
    "                            graph_X, graph_Y, graph_WH = graph_X.to(device), graph_Y.to(device), graph_WH.to(device)\n",
    "\n",
    "                            outputs, weights = net(node_features, graph_X, graph_Y, graph_WH)  \n",
    "                            group_outputs[count] = outputs\n",
    "                            \n",
    "                            count += 1                                            \n",
    "                        \n",
    "                        torch.cuda.empty_cache()\n",
    "                        _, predicted = torch.max(group_outputs.data, 1)\n",
    "                        c = (predicted == clear_labels).squeeze() \n",
    "                        total += clear_labels.size(0)\n",
    "                        correct += (predicted == clear_labels).sum()                       \n",
    "                        if  clear_labels.shape[0] == 1:\n",
    "                            class_correct[clear_labels] += c\n",
    "                        else:\n",
    "                            for j in range(clear_labels.shape[0]):  \n",
    "                                label = clear_labels[j] \n",
    "                                class_correct[label] += c[j]\n",
    "                                class_total[label] += 1    \n",
    "                        \n",
    "                    for j in range(3):\n",
    "                        print('Accuracy of %5s : %.3f %%' % (\n",
    "                                classes[j], 100 * class_correct[j] / class_total[j]))\n",
    "                    print('Acc on Validation set:：%.3f%%' % (100 * correct / total))\n",
    "                    acc = 100. * correct / total\n",
    "                    f.write(\"EPOCH=%03d,Accuracy= %.3f%%\" % (epoch + 1, acc))\n",
    "                    f.write('\\n')\n",
    "                    f.flush()\n",
    "                    if acc > best_acc:\n",
    "                        print('Saving model......')\n",
    "                        torch.save(net.state_dict(), '../../Trained/AffectNet_GAF_2.pth')\n",
    "                        f3 = open(\"best_acc.txt\", \"w\")\n",
    "                        f3.write(\"EPOCH=%d,best_acc= %.3f%%\" % (epoch + 1, acc))\n",
    "                        f3.close()\n",
    "                        best_acc = acc\n",
    "\n",
    "            print(\"Training Finished, TotalEPOCH=%d\" % EPOCH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose_pytorch",
   "language": "python",
   "name": "pose_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
